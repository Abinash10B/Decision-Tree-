{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30c0060c-48c0-475f-a568-ad0ab7887533",
   "metadata": {},
   "source": [
    "1.What is a Decision Tree, and how does it work in the context of classification?\n",
    "Ans->A **Decision Tree** is a supervised machine learning algorithm used for both classification and regression tasks. In the context of **classification**, it is used to predict a categorical outcome (such as Yes/No, Spam/Not Spam, Pass/Fail) by splitting the dataset into smaller subsets based on feature values. It resembles a tree-like structure where decisions are made step by step.\n",
    "\n",
    "A decision tree consists of three main components: **root node**, **internal nodes**, and **leaf nodes**. The root node is the top of the tree and represents the entire dataset. Internal nodes represent decision points where the data is split based on a specific feature and condition (for example, “Is age > 30?”). Leaf nodes represent the final output or class label.\n",
    "\n",
    "The tree works by repeatedly splitting the data into branches. At each step, the algorithm selects the feature that best separates the data into different classes. This selection is made using criteria such as **Gini Impurity** or **Entropy (Information Gain)**. These measures help determine how “pure” a node is—meaning how well the data points in that node belong to a single class. The goal is to create splits that increase purity and reduce uncertainty.\n",
    "\n",
    "For example, suppose we want to classify whether a person will buy a product. The decision tree might first split based on income level, then on age group, and then on location. At each split, the data becomes more specific until it reaches a leaf node that assigns a final class, such as “Buy” or “Not Buy.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777fa091-97fa-4e8e-a8a4-5981b16b6b9e",
   "metadata": {},
   "source": [
    "2.Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
    "Ans->In a Decision Tree for classification, **Gini Impurity** and **Entropy** are measures used to evaluate how “pure” or “impure” a node is. A node is considered pure if all the data points in it belong to the same class. The main goal of the decision tree algorithm is to create splits that reduce impurity as much as possible, leading to more homogeneous (pure) child nodes.\n",
    "\n",
    "**Gini Impurity** measures the probability that a randomly selected data point would be incorrectly classified if it were randomly labeled according to the class distribution in that node. Its formula is:\n",
    "\n",
    "[\n",
    "Gini = 1 - \\sum p_i^2\n",
    "]\n",
    "\n",
    "where ( p_i ) is the probability of class ( i ) in the node. If all samples in a node belong to one class, the Gini impurity becomes 0, meaning the node is perfectly pure. Higher values indicate more mixed classes. Decision trees using Gini impurity (like CART) choose the split that results in the largest reduction in Gini value.\n",
    "\n",
    "**Entropy**, on the other hand, comes from information theory and measures the level of uncertainty or randomness in the data. Its formula is:\n",
    "\n",
    "[\n",
    "Entropy = - \\sum p_i \\log_2(p_i)\n",
    "]\n",
    "\n",
    "When all samples belong to one class, entropy is 0 (no uncertainty). When classes are equally mixed, entropy is at its maximum. In decision trees, we calculate **Information Gain**, which is the reduction in entropy after a split. The algorithm selects the feature that provides the highest information gain (i.e., the greatest reduction in entropy).\n",
    "\n",
    "Both Gini impurity and entropy serve the same purpose: to determine the best feature and threshold for splitting the data at each node. The split that produces child nodes with lower impurity (higher purity) is preferred. In practice, both measures often produce similar trees, although Gini impurity is slightly faster to compute, while entropy provides a more theoretical measure of information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175102f-d1e3-451a-ba97-971656d1469d",
   "metadata": {},
   "source": [
    "3.What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
    "Ans->In Decision Trees, **pruning** is the process of reducing the size of the tree to prevent overfitting and improve generalization. There are two main types of pruning: **Pre-Pruning (Early Stopping)** and **Post-Pruning (Late Pruning)**. The key difference between them lies in *when* the pruning is applied during the tree-building process.\n",
    "\n",
    "**Pre-Pruning** stops the growth of the tree before it becomes too complex. In this approach, certain stopping conditions are set in advance, such as maximum depth of the tree, minimum number of samples required to split a node, or minimum impurity decrease. If these conditions are met, the tree stops splitting further. This means the tree is restricted while it is being built.\n",
    "\n",
    "A practical advantage of pre-pruning is that it **reduces computational cost and training time**, especially for large datasets. Since the tree stops growing early, it becomes simpler and faster to build, and it also reduces the risk of severe overfitting.\n",
    "\n",
    "**Post-Pruning**, on the other hand, allows the tree to grow fully (or almost fully) first. After the complete tree is constructed, branches that do not contribute significantly to predictive performance are removed. This is typically done using validation data or techniques like cost-complexity pruning, where subtrees are replaced with leaf nodes if doing so improves model performance on unseen data.\n",
    "\n",
    "A practical advantage of post-pruning is that it **often produces more accurate and better-generalized models**, because the tree initially captures all possible patterns before removing only the unnecessary or noisy branches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65817dcd-4668-447c-8633-25145e8a883d",
   "metadata": {},
   "source": [
    "4.What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
    "ANs->**Information Gain** is a metric used in Decision Trees to decide which feature should be used to split the data at a particular node. It is based on the concept of **Entropy**, which measures the amount of uncertainty or impurity in a dataset.\n",
    "\n",
    "Information Gain measures the **reduction in entropy** after a dataset is split on a particular feature. In simple terms, it tells us how much “information” a feature provides about the class label. The formula for Information Gain is:\n",
    "\n",
    "[\n",
    "Information\\ Gain = Entropy(parent) - \\sum \\left( \\frac{n_i}{n} \\times Entropy(child_i) \\right)\n",
    "]\n",
    "\n",
    "Here,\n",
    "\n",
    "* **Entropy(parent)** is the impurity before the split.\n",
    "* **Entropy(childᵢ)** is the impurity of each child node after the split.\n",
    "* ( \\frac{n_i}{n} ) is the proportion of samples in each child node.\n",
    "\n",
    "If a split greatly reduces entropy, the Information Gain will be high. If the split does not reduce uncertainty much, the Information Gain will be low.\n",
    "\n",
    "Information Gain is important because it helps the decision tree choose the **best possible split at each step**. The algorithm selects the feature that gives the **highest Information Gain**, meaning it produces the purest child nodes and reduces uncertainty the most. This ensures that the tree separates the classes effectively and builds a clear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d95311-e50d-4100-a424-3f3347cd3ada",
   "metadata": {},
   "source": [
    "5.What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
    "Ans->Decision Trees are widely used in real-world applications because they are simple, interpretable, and effective for both classification and regression tasks. One common application is in **banking and finance**, where decision trees are used for credit risk assessment—for example, deciding whether to approve or reject a loan based on income, credit score, and repayment history. In **healthcare**, they are used to assist in disease diagnosis by analyzing symptoms, test results, and patient history to classify whether a patient has a particular condition. In **marketing**, companies use decision trees for customer segmentation and predicting whether a customer will purchase a product. They are also used in **fraud detection**, spam email classification, employee attrition prediction, and even in recommendation systems.\n",
    "\n",
    "One of the main advantages of Decision Trees is that they are **easy to understand and interpret**. The tree structure visually represents decision rules, making it simple for non-technical users to follow the reasoning process. They can handle both numerical and categorical data without requiring much preprocessing. Additionally, decision trees do not require feature scaling and can capture non-linear relationships between variables.\n",
    "\n",
    "However, Decision Trees also have some limitations. A major drawback is that they are prone to **overfitting**, especially when the tree becomes very deep and complex. This can cause the model to perform well on training data but poorly on unseen data. They can also be **unstable**, meaning small changes in the dataset can result in a completely different tree structure. Furthermore, individual decision trees may not always provide the highest predictive accuracy compared to more advanced ensemble methods like Random Forest or Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f5edef-01e1-4a8c-a09c-8430c869afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n",
      "\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0\n",
      "sepal width (cm): 0.01911001911001911\n",
      "petal length (cm): 0.8932635518001373\n",
      "petal width (cm): 0.08762642908984374\n"
     ]
    }
   ],
   "source": [
    "#6.   Write a Python program to: ● Load the Iris Dataset ● Train a Decision Tree Classifier using the Gini criterion ● Print the model’s accuracy and feature importances.\n",
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris Dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train Decision Tree Classifier using Gini criterion\n",
    "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5. Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "# 6. Print feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(iris.feature_names, model.feature_importances_):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff92bd7-7346-48e6-bef4-7f1dd5c67eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully-Grown Tree Accuracy: 1.0\n",
      "Max Depth = 3 Tree Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#7.Write a Python program to: ● Load the Iris Dataset ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris Dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train a fully-grown Decision Tree\n",
    "full_tree = DecisionTreeClassifier(random_state=42)\n",
    "full_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and accuracy\n",
    "y_pred_full = full_tree.predict(X_test)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "# 4. Train a Decision Tree with max_depth=3\n",
    "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "limited_tree.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and accuracy\n",
    "y_pred_limited = limited_tree.predict(X_test)\n",
    "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
    "\n",
    "# 5. Print results\n",
    "print(\"Fully-Grown Tree Accuracy:\", accuracy_full)\n",
    "print(\"Max Depth = 3 Tree Accuracy:\", accuracy_limited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa22bef2-428d-4311-84f0-69a0d09d7f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 11.588026315789474\n",
      "\n",
      "Feature Importances:\n",
      "CRIM: 0.05846545229060361\n",
      "ZN: 0.000988919249451643\n",
      "INDUS: 0.009872448809169472\n",
      "CHAS: 0.0002973342835618114\n",
      "NOX: 0.007050562083191356\n",
      "RM: 0.575807411273885\n",
      "AGE: 0.007170198655228184\n",
      "DIS: 0.10962404854314393\n",
      "RAD: 0.001646356693641641\n",
      "TAX: 0.002181112508453187\n",
      "PTRATIO: 0.025042865841170155\n",
      "B: 0.011872990423277916\n",
      "LSTAT: 0.189980299345222\n"
     ]
    }
   ],
   "source": [
    "#8.Write a Python program to: ● Load the Boston Housing Dataset ● Train a Decision Tree Regressor ● Print the Mean Squared Error (MSE) and feature importances.\n",
    "# Import required libraries\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the Boston Housing Dataset\n",
    "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
    "\n",
    "X = boston.data\n",
    "y = boston.target.astype(float)  # Convert target to numeric\n",
    "\n",
    "# 2. Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train Decision Tree Regressor\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5. Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# 6. Print feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(X.columns, model.feature_importances_):\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50b9857-8a73-4fe6-a525-aa699a351c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_split': 10}\n",
      "Model Accuracy with Best Parameters: 1.0\n"
     ]
    }
   ],
   "source": [
    "#9.Write a Python program to: ● Load the Iris Dataset ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV ● Print the best parameters and the resulting model accuracy.\n",
    "# Import required libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load the Iris Dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2. Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Define the model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# 4. Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [None, 2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# 5. Apply GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 6. Get best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 7. Evaluate on test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 8. Print results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Model Accuracy with Best Parameters:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca26bf1-28ed-4f64-9f4a-67b3542c0b09",
   "metadata": {},
   "source": [
    "10. Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to: ● Handle the missing values ● Encode the categorical features ● Train a Decision Tree model ● Tune its hyperparameters ● Evaluate its performance And describe what business value this model could provide in the real-world setting.\n",
    "Ans->If I were working as a data scientist for a healthcare company trying to predict whether a patient has a certain disease, I would follow a structured and systematic process to ensure the model is accurate, reliable, and useful in a real-world setting.\n",
    "\n",
    "First, I would begin with **data understanding and preprocessing**, especially handling missing values. In healthcare datasets, missing values are common due to incomplete patient records or skipped medical tests. I would first analyze the percentage and pattern of missing data. If a feature has too many missing values (for example, more than 40–50%), I might consider removing that feature if it does not carry critical medical importance. For numerical features such as blood pressure or cholesterol levels, I would typically use imputation techniques like replacing missing values with the median (which is robust to outliers). For categorical variables such as gender or smoking status, I would use the most frequent value (mode) or introduce a separate category like “Unknown.” If the dataset is large and complex, more advanced techniques such as KNN imputation could also be considered. The key goal is to retain as much useful information as possible without introducing bias.\n",
    "\n",
    "Next, I would **encode categorical features**. Since Decision Trees can handle numerical inputs directly but not raw text categories, categorical variables such as gender, region, or test result categories must be converted into numeric format. For binary categories (e.g., Male/Female), I would use label encoding (0 and 1). For features with multiple categories (e.g., blood type or city), I would use one-hot encoding to avoid introducing artificial ordinal relationships. Care would be taken to prevent multicollinearity and data leakage during encoding, especially when splitting training and testing data.\n",
    "\n",
    "After preprocessing, I would **train a Decision Tree classifier**. I would split the dataset into training and testing sets (for example, 70% training and 30% testing). Then I would train the Decision Tree model on the training data. Decision Trees are particularly useful in healthcare because they are interpretable. Doctors and stakeholders can easily understand decision rules such as: “If age > 50 and cholesterol > X, then high risk.”\n",
    "\n",
    "However, a default Decision Tree can easily overfit the training data. Therefore, I would proceed to **hyperparameter tuning**. Using techniques such as GridSearchCV or RandomizedSearchCV, I would tune parameters like `max_depth` (maximum depth of the tree), `min_samples_split`, `min_samples_leaf`, and possibly `criterion` (Gini or Entropy). Cross-validation would be used to ensure the model generalizes well across different subsets of the data. The goal is to balance bias and variance—avoiding both underfitting and overfitting.\n",
    "\n",
    "Once the best model is selected, I would **evaluate its performance**. Since this is a disease prediction problem (a classification task), accuracy alone is not enough. In healthcare, metrics like precision, recall, F1-score, and ROC-AUC are very important. For example, recall (sensitivity) is crucial because we want to minimize false negatives (patients who have the disease but are predicted as healthy). A confusion matrix would also help analyze false positives and false negatives. If the disease is rare, I would also check for class imbalance and possibly apply techniques like SMOTE or class weighting.\n",
    "\n",
    "Finally, from a **business perspective**, this model could provide significant value. It could help in early disease detection, allowing doctors to prioritize high-risk patients for further testing. This reduces diagnostic delays and improves patient outcomes. It can also optimize hospital resource allocation by identifying patients who require urgent care. Additionally, insurance companies or healthcare providers could use the model to design preventive care programs, reducing long-term treatment costs. Because Decision Trees are interpretable, healthcare professionals can understand and trust the model’s decision-making process, which is critical in medical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5fb293-7255-44a2-8f04-d4940cdf82c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
